{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/BalintVargaBME/Deep-Learning-a-gyarkolatban---BitMesterek/blob/main/BitMesterek_Optiver_Trading_at_the_close.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MWF0D3t5_Wxo"
      },
      "outputs": [],
      "source": [
        "# Import libraries\n",
        "import pandas as pd\n",
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import random\n",
        "from lightgbm import LGBMRegressor\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "klruvZKRCKkR"
      },
      "outputs": [],
      "source": [
        "# Mount drive to colab\n",
        "#from google.colab import drive\n",
        "#drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Wcasjs8hCROj"
      },
      "outputs": [],
      "source": [
        "# Navigate to working directory\n",
        "# Change to your own Drive directory after MyDrive/\n",
        "\n",
        "#%cd /content/drive/MyDrive/Deep Learning a gyakorlatban/Nagy házi/\n",
        "\n",
        "# Telling Kaggle where to find the API key, needed for dowloading dataset, change to your own directory after MyDrive/\n",
        "#os.environ['KAGGLE_CONFIG_DIR'] = \"/content/drive/MyDrive/Deep Learning a gyakorlatban/Nagy házi\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3G__JhZjAH02"
      },
      "outputs": [],
      "source": [
        "# Commented out so Colab won't download each time when all cells are run\n",
        "\n",
        "# Download data from Kaggle\n",
        "#!kaggle competitions download -c optiver-trading-at-the-close"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j3hUrr3vD9Go",
        "outputId": "01a8a57a-420d-4a37-dacb-e8d4f5936b2e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " \u001b[0m\u001b[01;34msample_data\u001b[0m/  'train.csv?rlkey=r7rwxzxrawaavj8uyhb4edx6j'   \u001b[01;34mtuning_directory\u001b[0m/\n"
          ]
        }
      ],
      "source": [
        "# Commented out so Colab won't download each time when all cells are run\n",
        "# Unzip, then delete file\n",
        "\n",
        "\n",
        "#for file in os.listdir():\n",
        " #   if file.endswith(\".zip\"):\n",
        "  #      with zipfile.ZipFile(file, \"r\") as zip_file:\n",
        "   #         zip_file.extractall()\n",
        "\n",
        "\n",
        "# List files in working directory\n",
        "%ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PoyAhmZWqMhh"
      },
      "outputs": [],
      "source": [
        "# Download train.csv\n",
        "!wget https://www.dropbox.com/scl/fi/ibujwdcz78j7u2svlqmum/train.csv?rlkey=r7rwxzxrawaavj8uyhb4edx6j&dl=0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DZ9DyY8DFZda"
      },
      "outputs": [],
      "source": [
        "\n",
        "import_data = pd.read_csv ('train.csv?rlkey=r7rwxzxrawaavj8uyhb4edx6j')\n",
        "import_data.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "o6PB-vEcca5f"
      },
      "source": [
        "Number of datapoints:\n",
        "\n",
        "200 stocks\n",
        "481 days\n",
        "55 steps each day\n",
        "\n",
        "So number of datapoints should be 200 * 481 * 55  (stocks * days * steps) = 5291000"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M8BpcMYnIhp7"
      },
      "outputs": [],
      "source": [
        "\n",
        "import_data.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n9M0ZQiYdxXd"
      },
      "source": [
        "We can see that 53020 datapoints are missing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "K0iIkCAGd2bO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fe64c831-31c3-4dc3-96b8-7778e03eb7bf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Date and number of missing stocks:\n",
            "date_id\n",
            "0      9\n",
            "1      9\n",
            "2      9\n",
            "3      9\n",
            "4      8\n",
            "      ..\n",
            "476    0\n",
            "477    0\n",
            "478    0\n",
            "479    0\n",
            "480    0\n",
            "Name: stock_id, Length: 481, dtype: int64\n",
            "Total number of missing stocks: 964\n"
          ]
        }
      ],
      "source": [
        "\n",
        "data_frame = import_data\n",
        "\n",
        "# Group the data by 'date_id' and count the unique 'stock_id' values for each day\n",
        "stock_count_per_day = data_frame.groupby('date_id')['stock_id'].nunique()\n",
        "\n",
        "# Expected number of stocks each day\n",
        "expected_stock_count = 200\n",
        "\n",
        "# Number of missing stocks each day\n",
        "missing_stock_count = expected_stock_count - stock_count_per_day\n",
        "\n",
        "# Total missing stocks througouth the 200 days\n",
        "total_missing_stocks = missing_stock_count.sum()\n",
        "\n",
        "# Days and number of missing stocks\n",
        "print(\"Date and number of missing stocks:\")\n",
        "print(missing_stock_count)\n",
        "\n",
        "# Total number of missing stocks\n",
        "print(\"Total number of missing stocks:\", total_missing_stocks)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sQKgAmopmOGN"
      },
      "source": [
        "As we can see throughout the 481 days a total of 964 stocks are missing entirely. 964 * 55 = 53020,  so this means that missing data is found. This also means that fortunately that the stock data that we do have doesn't have any missing values."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rPbIVSf9zOrz"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Function to plot the time series of a stock on a day\n",
        "def stock_plotter(data_frame, cols, stock_id, date_id):\n",
        "\n",
        "  # Filtering data_frame where the stock_id and date_id match with our inputs\n",
        "  data_frame = data_frame.query(f'stock_id == {stock_id} & date_id == {date_id}')\n",
        "\n",
        "  # We want all rows of the seconds_in_bucket and input columns\n",
        "  data_frame = data_frame.loc[:, ['seconds_in_bucket'] + cols]\n",
        "\n",
        "  # Set seconds_in_bucket as the index for the data_frame\n",
        "  data_frame = data_frame.set_index('seconds_in_bucket')\n",
        "\n",
        "  # Plot time series\n",
        "  data_frame = data_frame.plot(title=f'Stock {stock_id} on Day {date_id}', figsize=(10, 4), linewidth=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nhTJzBTX1No2"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Generate random numbers to print random stocks on random days, not excluding the missing ones (will have to fix in future)\n",
        "stock_no = random.randint(0,200)\n",
        "day_no = random.randint(0,481)\n",
        "\n",
        "# Plot\n",
        "stock_plotter(\n",
        "  data_frame=import_data,\n",
        "  cols=['bid_price','ask_price', 'wap'],\n",
        "  stock_id=stock_no,\n",
        "  date_id=day_no)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yrdikP85q0kD"
      },
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "# Dropping NaN values because they cause errors in calculatin MAE\n",
        "data_frame = data_frame.dropna(subset=['target'])\n",
        "\n",
        "# Splitting data, dropping row_id becaues it's not necesseary, and it's 'object' type causing error with pandas\n",
        "#X = data_frame.drop(['target', 'row_id'], axis=1)\n",
        "#y = data_frame[['target']]\n",
        "\n",
        "\n",
        "#X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1,)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_H5FDS-Q_6yK"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Storing hyperparameters and MAE score, will have better system for it in future\n",
        "#model_params = []\n",
        "#model_mae = []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LXWEvIACAf35"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Using LGBM framework for training model\n",
        "#from lightgbm import LGBMRegressor\n",
        "#params={'n_estimators': 300,\n",
        " #       'learning_rate': 0.09,\n",
        "  #      'max_depth': 8,\n",
        "   #     'num_leaves': 25,\n",
        "    #    }\n",
        "\n",
        "#model=LGBMRegressor(**params, random_state=123, device='cpu')\n",
        "#model.fit(X_train, y_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q-Zxg8quA6Q9"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Get predictions of test set\n",
        "#pred_LGB=model.predict(X_test)\n",
        "\n",
        "# Calculate the mean absolute error of the predictions and target\n",
        "#mean_ae = mean_absolute_error(y_test,pred_LGB)\n",
        "#print(\"MAE score of model =\", mean_ae)\n",
        "\n",
        "# Store hyperparameters and score\n",
        "#model_params.append(model)\n",
        "#model_mae.append(mean_ae)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GNyWU0TDHlmm"
      },
      "outputs": [],
      "source": [
        "\n",
        "#print(model_params, model_mae)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!pip install keras_tuner"
      ],
      "metadata": {
        "id": "CB5aKppn-0mo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#There are a lot of Nan values in the dataset (especially in far_price, near_price), which causes problems with the training\n",
        "print(data_frame.isnull().sum())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-qYhTPQYqxb",
        "outputId": "b219ef39-90b5-4bd8-c5db-9bcc6dfcc323"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "stock_id                         0\n",
            "date_id                          0\n",
            "seconds_in_bucket                0\n",
            "imbalance_size                 132\n",
            "imbalance_buy_sell_flag          0\n",
            "reference_price                132\n",
            "matched_size                   132\n",
            "far_price                  2894254\n",
            "near_price                 2857092\n",
            "bid_price                      132\n",
            "bid_size                         0\n",
            "ask_price                      132\n",
            "ask_size                         0\n",
            "wap                            132\n",
            "target                           0\n",
            "time_id                          0\n",
            "row_id                           0\n",
            "dtype: int64\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#delete the rows where these features are NaN (only 132 each which is not a big loss, considering that the number of all rows are 2894254)\n",
        "data_frame = data_frame.dropna(subset=['imbalance_size', 'reference_price', 'matched_size', 'bid_price', 'ask_price', 'wap'])"
      ],
      "metadata": {
        "id": "2Yix8pb5Yp9n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''#now far_price and near_price are the only features which have NaN values\n",
        "#I am going to predict the missing values with Linear Regression (later can be improved)\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "data_frame_temp = data_frame.dropna(subset=['far_price', 'near_price'])\n",
        "\n",
        "# Split the data into features (X) and target variable (y)\n",
        "X_temp = data_frame_temp.drop(['far_price', 'near_price'], axis=1)\n",
        "y_far = data_frame_temp[['far_price']]\n",
        "y_near = data_frame_temp[['near_price']]\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_far_train, X_far_test, y_far_train, y_far_test = train_test_split(X_temp, y_far, test_size=0.2, random_state=42)\n",
        "X_near_train, X_near_test, y_near_train, y_near_test = train_test_split(X_temp, y_near, test_size=0.2, random_state=42)\n",
        "\n",
        "# Train the Linear Regression model, later can be improved\n",
        "model_far = LinearRegression()\n",
        "model_far.fit(X_far_train, y_far_train)\n",
        "\n",
        "model_near = LinearRegression()\n",
        "model_near.fit(X_near_train, y_near_train)\n",
        "\n",
        "# Make predictions\n",
        "y_far_pred = model_far.predict(X_far_test)\n",
        "y_near_pred = model_near.predict(X_near_test)\n",
        "\n",
        "# Evaluate the model\n",
        "mse_far = mean_squared_error(y_far_test, y_far_pred)\n",
        "mse_near = mean_squared_error(y_near_test, y_near_pred)\n",
        "\n",
        "print(f'Mean Squared Error (far_price): {mse_far}')\n",
        "print(f'Mean Squared Error (near_price): {mse_near}')'''"
      ],
      "metadata": {
        "id": "ciC_5dph_ec0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "'''rows_with_missing_values_far = data_frame[data_frame['far_price'].isnull()]\n",
        "rows_with_missing_values_near = data_frame[data_frame['near_price'].isnull()]\n",
        "\n",
        "rows_with_missing_values_far = rows_with_missing_values_far.drop(['far_price', 'near_price'], axis=1)\n",
        "rows_with_missing_values_near = rows_with_missing_values_near.drop(['far_price', 'near_price'], axis=1)\n",
        "\n",
        "# Use the trained model to predict missing values\n",
        "predicted_far_values = model_far.predict(rows_with_missing_values_far)\n",
        "predicted_near_values = model_near.predict(rows_with_missing_values_near)\n",
        "\n",
        "# Fill in the missing values in the original DataFrame\n",
        "data_frame.loc[data_frame['far_price'].isnull(), 'far_price'] = predicted_far_values\n",
        "data_frame.loc[data_frame['near_price'].isnull(), 'near_price'] = predicted_near_values\n",
        "\n",
        "print(data_frame.isnull().sum())'''"
      ],
      "metadata": {
        "id": "NHZNR23_DFeH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install xgboost"
      ],
      "metadata": {
        "id": "kzL6MEduAy6N"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import xgboost as xgb\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import mean_squared_error\n",
        "\n",
        "def predict_missing_values(data_frame, target_column):\n",
        "    # Filter out non-numeric columns\n",
        "    numeric_df = data_frame.select_dtypes(include=['int', 'float', 'bool'])\n",
        "\n",
        "    # Split the data into subsets\n",
        "    data_with_target = numeric_df[numeric_df[target_column].notnull()]\n",
        "    data_without_target = numeric_df[numeric_df[target_column].isnull()]\n",
        "\n",
        "    # Prepare feature matrix (X) and target vector (y)\n",
        "    X = data_with_target.drop(columns=[target_column])\n",
        "    y = data_with_target[target_column]\n",
        "\n",
        "    # Split the data into training and testing sets\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Initialize and train the XGBoost model\n",
        "    model = xgb.XGBRegressor(objective='reg:squarederror', n_estimators=100, learning_rate=0.1, max_depth=5)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Evaluate the model\n",
        "    y_pred = model.predict(X_test)\n",
        "    mse = mean_squared_error(y_test, y_pred)\n",
        "    print(f\"Mean Squared Error for {target_column}: {mse}\")\n",
        "\n",
        "    # Predict missing values\n",
        "    X_missing = data_without_target.drop(columns=[target_column])\n",
        "    predicted_values = model.predict(X_missing)\n",
        "\n",
        "    # Fill in the missing values in the original DataFrame\n",
        "    data_frame.loc[data_frame[target_column].isnull(), target_column] = predicted_values\n",
        "\n",
        "# Predict and impute missing values\n",
        "predict_missing_values(data_frame, 'far_price')\n",
        "predict_missing_values(data_frame, 'near_price')\n",
        "\n",
        "# Check remaining missing values\n",
        "print(data_frame.isnull().sum())"
      ],
      "metadata": {
        "id": "-EHbHughA0dc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting data, dropping row_id becaues it's not necesseary, and it's 'object' type causing error with pandas\n",
        "X = data_frame.drop(['target', 'row_id'], axis=1)\n",
        "y = data_frame[['target']]\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1, random_state=42)"
      ],
      "metadata": {
        "id": "2rg11yZh6spM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from keras_tuner.tuners import Hyperband\n",
        "\n",
        "import os\n",
        "from tensorflow.keras.models import load_model\n",
        "from sklearn.metrics import mean_absolute_error\n",
        "\n",
        "#predicting with Hyperband\n",
        "\n",
        "def build_model(hp):\n",
        "    model_new = keras.Sequential()\n",
        "\n",
        "     # Input layer\n",
        "    model_new.add(layers.Flatten(input_shape=(X_train.shape[1],)))\n",
        "\n",
        "    # Hidden layers\n",
        "    for i in range(hp.Int('num_layers', min_value=2, max_value=4, step=1)):\n",
        "        model_new.add(layers.Dense(units=hp.Int('units_' + str(i), min_value=32, max_value=512, step=32), activation='relu'))\n",
        "        model_new.add(layers.Dropout(rate=hp.Float('dropout_' + str(i), min_value=0.0, max_value=0.5, step=0.1)))\n",
        "\n",
        "    # Output layer\n",
        "    model_new.add(layers.Dense(1, activation='linear'))\n",
        "\n",
        "\n",
        "    '''model_new.compile(optimizer=keras.optimizers.Adam(hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])),\n",
        "                  loss='mean_squared_error',\n",
        "                  metrics=['mean_absolute_error'])\n",
        "    '''\n",
        "\n",
        "    model_new.compile(\n",
        "        optimizer=hp.Choice('optimizer', values=['adam', 'sgd', 'rmsprop']),\n",
        "        loss='mean_squared_error',\n",
        "        metrics=['mean_absolute_error'])\n",
        "    return model_new\n",
        "\n",
        "\n",
        "input_shape = X_train.shape[1]\n",
        "output_classes = 1\n",
        "\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n",
        "\n",
        "tuner = Hyperband(\n",
        "    build_model,\n",
        "    #ideally it should be 1<maX but it is rather time consuming\n",
        "    max_epochs=10,\n",
        "    factor=3,\n",
        "    objective='val_mean_absolute_error',\n",
        "    directory='tuning_directory',\n",
        "    project_name='project'\n",
        ")\n",
        "\n",
        "tuner.search(X_train, y_train, epochs = 2, validation_data=(X_val, y_val), callbacks=[early_stopping])"
      ],
      "metadata": {
        "id": "Vdpg_A1fA7-k",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "#best_model = tuner.get_best_models(num_models=1)[0]\n",
        "#best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# First, retrieve the best hyperparameters\n",
        "#best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]\n",
        "\n",
        "# Then, manually build the model with these hyperparameters\n",
        "#best_model = build_model(best_hyperparameters)\n",
        "best_model = tuner.get_best_models(num_models=1)[0]\n",
        "\n",
        "# Retrain this model\n",
        "#best_model.fit(X_train, y_train, epochs=2, validation_data=(X_val, y_val), callbacks=[early_stopping])\n",
        "\n",
        "# Function to evaluate model performance\n",
        "def evaluate_model(model, X_test, y_test):\n",
        "    predictions = model.predict(X_test)\n",
        "    return mean_absolute_error(y_test, predictions)\n",
        "\n",
        "\n",
        "# Evaluate the manually built and trained model\n",
        "best_model_mae = evaluate_model(best_model, X_test, y_test)\n",
        "# Retrieve the best model from the tuner\n",
        "#new_model = tuner.get_best_models(num_models=1)[0]\n",
        "#new_model_mae = evaluate_model(new_model, X_test, y_test)"
      ],
      "metadata": {
        "id": "LGvt7O0wu7Mm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the existing best model if it exists\n",
        "model_file = 'best_model.h5'\n",
        "if os.path.exists(model_file):\n",
        "    saved_model = load_model(model_file)\n",
        "    saved_model_mae = evaluate_model(saved_model, X_test, y_test)\n",
        "else:\n",
        "    saved_model_mae = float('inf')  # Set to infinity if no model is saved"
      ],
      "metadata": {
        "id": "ihu9gE0-RdDG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Save the new model only if it performs better\n",
        "if best_model_mae < saved_model_mae:\n",
        "    best_model.save(model_file)\n",
        "    print(\"New model saved as it performs better.\")\n",
        "    #best_model = new_model\n",
        "    #best_model_mae = new_model_mae\n",
        "else:\n",
        "    print(\"Existing model retained as it performs better.\")\n",
        "    best_model = saved_model if os.path.exists(model_file) else new_model\n",
        "    best_model_mae = saved_model_mae if os.path.exists(model_file) else new_model_mae\n",
        "\n",
        "# Print out the overall best model's performance\n",
        "print(\"Overall Best Model MAE:\", best_model_mae)"
      ],
      "metadata": {
        "id": "55PBfroLTrCu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "best_model.fit(X_train, y_train)\n",
        "\n",
        "# Get predictions of test set\n",
        "pred_LGB=best_model.predict(X_test)\n",
        "\n",
        "# Calculate the mean absolute error of the predictions and target\n",
        "mean_ae = mean_absolute_error(y_test,pred_LGB)\n",
        "print(\"MAE score of model =\", mean_ae)"
      ],
      "metadata": {
        "id": "mjHpqH7KFAmW"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}